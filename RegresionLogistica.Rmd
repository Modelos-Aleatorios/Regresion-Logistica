---
title: "Regresión Logística"
author: "Alexander A. Ramírez M. (alexanderramirez.me) y Daysi Febles (daysilorenafeblesr@gmail.com)"
date: "27/3/2017"
output:
  pdf_document: default
  html_document: default
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#https://github.com/stefano-meschiari/latex2exp
#install.packages('latex2exp')
library(latex2exp)
```

## Marco Teórico

Para predecir el comportamiento de una variable que depende de otras variables es usual la aplicación de los métodos de regresión. Entre ellos el más popular es la regresión lineal. El proceso consiste en encontrar los parámetros de una función lineal (una recta en este caso) que mejor se ajusta a los datos (con la menor distancia). El criterio para encontrar la mejor, básicamente, consiste en minimizar los errores y para ellos se utiliza el método de mínimos cuadrados.

Hay un caso en el cual la regresión lineal no se comporta apropiadamente, y este es el caso donde la variable de respuesta es dicotómica (ocurra o no algún evento). Veamos a continuación porque pasa esto: 

Supongamos que queremos predecir el valor de la variable de respuesta $Y$ dicotómica en un individuo de la población en función de ciertas características medibles $X_1,X_2,...,X_n$. 

Comenzaremos ajustando un modelo de regresión lineal simple cuando tenemos una sola variable explicativa, que llamaremos $X$ y que puede ser numérica o categórica, es decir
$$Y=\beta_0 + \beta_1X+\epsilon$$
$$\epsilon \sim \mathcal{N}(0,\sigma)$$

Como la variable respuesta es binaria (toma valor 0 o 1) diremos que es una variable aleatoria Bernoulli y con respecto a la $X$ es una variable observada, sus valores son conocidos. Asumiremos que tenemos $n$ elementos de ambas $(x_i,y_i)$ con $i=1,2,...,n$. $\epsilon$ representara el error aleatorio.

Observemos los inconvenientes de este modelo:

- Cuando calculamos la esperanza del modelo tenemos
$$
\begin{array}{rl}
E(Y_i) & = E(\beta_0 + \beta_1X_i+\epsilon_i)\\
E(Y_i) & = E(\beta_0) + E(\beta_1X_i)+E(\epsilon_i)\\
E(Y_i) & = \beta_0 + \beta_1X_i
\end{array}
$$

Si escribimos el modelo como $\beta_0 + \beta_1X_i=p_i$ donde $p_i=P(Y_i=1)$
$$Y_i= \left \{ \begin{matrix} 1 \  \mbox{con probabilidad} \ p_i\\ 0 \ \mbox{con probabilidad} \ 1-p_i  \end{matrix} \right.$$

Así este modelo estimaría la probabilidad de que ocurra el evento $Y_i$. Como $p_i$ es una probabilidad su valor deberia estar entre 0 y 1 $(0\le p_i\le 1)$ para todo $i$. Pero como $X_i$, $\beta_0$ y $\beta_1$ pueden tomar cualquier valor entre $(-\infty,\infty)$ el modelo de regresión lineal no garantiza que siempre de un valor entre cero y uno.

- La variable de respuesta solo toma los valores cero y uno, ahora si conocemos la variable explicativa $X$ tenemos que:
$$
\begin{array}{rl}
\epsilon_i & = Y_i-\beta_0+\beta_1X_i\\
\epsilon_i & = \left \{ \begin{matrix} 1 -(\beta_0+\beta_1X_i)\ \mbox{con probabilidad} \ p_i \\ 0 -(\beta_0+\beta_1X_i)\ \mbox{con probabilidad} \ 1-p_i \end{matrix} \right.\\
\epsilon_i & = \left \{ \begin{matrix} 1 -(\beta_0+\beta_1X_i)\ \mbox{con probabilidad} \ p_i \\ -(\beta_0+\beta_1X_i)\ \mbox{con probabilidad} \ 1-p_i  \end{matrix} \right.\\
\epsilon_i & = \left \{ \begin{matrix} 1 -p_i\ \mbox{con probabilidad} \ p_i \\ -p_i\ \mbox{con probabilidad} \ 1-p_i  \end{matrix} \right.
\end{array}
$$

Así $\epsilon$ tendría una distribución discreta. Veamos cual es el valor esperado de esta variable aleatoria:
$$
\begin{array}{rl}
E(\epsilon_i) & = (1 -(\beta_0+\beta_1X_i))P(\epsilon_i=1 -(\beta_0+\beta_1X_i))+ (-(\beta_0+\beta_1X_i))P(\epsilon_i=-(\beta_0+\beta_1X_i))\\
E(\epsilon_i) & = (1 -p_i)P(\epsilon_i=1 -p_i)+ (-p_i)P(\epsilon_i=-p_i)\\
E(\epsilon_i) & = (1 -p_i)p_i-(p_i)(1-p_i)\\
E(\epsilon_i) & = p_i-p_i^2-p_i+p_i^2\\
E(\epsilon_i) & =0
\end{array}
$$

Aunque la esperanza de esta variable es cero, no cumple con el supuesto de normalidad de los errores, ya que toma valores discretos. Esto nos llevaría a estimadores poco eficientes.

- Varianza de los errores:

Para el cálculo de la varianza de los errores tenemos que definir primero $\epsilon_i^2$
$$
\begin{array}{rl}
\epsilon_i^2 & = \left \{ \begin{matrix} (1 -p_i)^2\ \mbox{con probabilidad} \ p_i \\ (-p_i)^2\ \mbox{con probabilidad} \ 1-p_i  \end{matrix} \right.\\
\epsilon_i^2 & = \left \{ \begin{matrix} (1 -p_i)^2\ \mbox{con probabilidad} \ p_i \\ p_i^2\ \mbox{con probabilidad} \ 1-p_i  \end{matrix} \right. 
\end{array}
$$

Así
$$
\begin{array}{rl}
Var(\epsilon_i) & = E(\epsilon_i^2)- E(\epsilon_i)^2\\
Var(\epsilon_i) & = E(\epsilon_i^2)\\
Var(\epsilon_i) & = (1 -p_i)^2P(\epsilon_i=(1 -p_i)^2)+p_i^2P(\epsilon_i=p_i^2)\\
Var(\epsilon_i) & = (1 -p_i)^2p_i+p_i^2(1-p_i)\\
Var(\epsilon_i) & = (1 -p_i)((1 -p_i)p_i+p_i^2)\\
Var(\epsilon_i) & = (1 -p_i)(p_i-p_i^2+p_i^2)\\
Var(\epsilon_i) & = (1 -p_i)p_i\\
Var(\epsilon_i) & = (1 -(\beta_0+\beta_1X_i))(\beta_0+\beta_1X_i)
\end{array}
$$

También contradice la hipótesis de los $\epsilon_i$, donde la varianza tiene que ser constante, acabamos de obtener una función que depende de la variable explicativa, por lo tanto no es constante.

Como vimos anteriormente el modelo lineal $\beta_0+\beta_1X_i$ no es bueno para predecir los valores de $p_i$. Para garantizar que este valor este entre cero y uno, tomaremos cualquier función de distribución $F$ y lo aproximaremos mediante
$$F(\beta_0+\beta_1X_i)=p_i\ \mbox{para}\ i=1,...,n$$
Veamos algunas funciones de distribución:

1) $F$ es una función de **distribución Uniforme** en $[0,1]$
$$p_i = \left \{ 
\begin{matrix} 
1  & \mbox{si}\ \beta_0+\beta_1X_i\ge 1 \\
\beta_0+\beta_1X_i &\mbox{si}\  0<\beta_0+\beta_1X_i<1 \\
0 & \mbox{si}\ \beta_0+\beta_1X_i\le 0
\end{matrix} \right. 
$$

2) $F$ es la función de **distribución Normal** (Modelo Probit)
$$p_i=\Phi(\beta_0+\beta_1X_i)$$

De donde $\Phi^{-1}(p_i)=\beta_0+\beta_1X_i$

3) $F$ es la función de **distribución logística**
$$p_i=\frac{1}{1+e^{-(\beta_0+\beta_1X_i)}}=g(X_i) \qquad (1)$$
Este modelo se conoce como **Modelo Logístico** o **Modelo Logit**.

A continuación utilizaremos el modelo logístico, veremos todas sus propiedades. 

Comenzaremos observando el gráfico de $g(x)=\frac{1}{1+e^{-x}}$

```{r}
x<-seq(-8,8,1)
y<-1/(1+exp(-x))
plot(x,y, main = TeX('Gráfico de $g(x)=\\frac{1}{1+e^{-x}}$'),
     xlab = TeX('x'),ylab = TeX('$g(x)$'), type="l",col="blue")
```

Observamos que toma valores entre 0 y 1. 

Veamos que la ecuación $(1)$ cumple con las propiedades siguientes:

1) $g(x_i)$ describe el intervalo $[0,1]$, excepto cuando $\beta_1=0$ ya que nos quedaría una constante.

2) $g(x_i)$ es monótona creciente cuando $\beta_1>0$ y monótona decreciente cuando $\beta_1<0$

3) $g(-\frac{\beta_0}{\beta_1})=\frac{1}{2}$ y es simétrica respecto al punto $(\frac{\beta_0}{\beta_1},\frac{1}{2})$ es decir $$g(-\frac{\beta_0}{\beta_1}-u)=1-g(-\frac{\beta_0}{\beta_1}+u) \ \forall u \in \mathbb{R}$$

A continuación probaremos estas tres propiedades:

1.- Si $\beta_1 \ne 0$, entonces tenemos los tres caso siguientes:
    
  - Si $\beta_0 + \beta_1 X_i \to \infty$ entonces $g(X_i)=\frac{1}{1+e^{-(\beta_0+\beta_1X_i)}} \to 1$
  - Si $\beta_0 + \beta_1 X_i \to 0$ entonces $g(X_i)=\frac{1}{1+e^{-(\beta_0+\beta_1X_i)}} \to \frac{1}{2}$
  - Si $\beta_0 + \beta_1 X_i \to -\infty$ entonces $g(X_i)=\frac{1}{1+e^{-(\beta_0+\beta_1X_i)}} \to 0$

La función $g$ es continua por ser composición de funciones continuas.

2.- Sea $g(X_i)=\frac{1}{1+e^{-(\beta_0+\beta_1X_i)}}=\frac{1}{1+e^{-\beta_0-\beta_1X_i}}$ y llamemos $C=e^{-\beta_0}$, notemos que $C$ es positiva para cualquier valor de $\beta_0$. Luego tenemos:
$$g(X_i)=\frac{1}{1+Ce^{-\beta_1X_i}}$$

Sabemos por el comportamiento de la función exponencial que:

  - Para $\beta_1>0$, si $a<b$ entonces $e^{-a\beta_1}>e^{-b\beta_1}$ por lo tanto
$$g(a)=\frac{1}{1+Ce^{-\beta_1a}}<\frac{1}{1+Ce^{-\beta_1b}}=g(b)$$

Así tenemos que $g(X_i)$ es monótona creciente si $\beta_1>0$.

  - Para $\beta_1<0$, si $a<b$ se tiene que $e^{-b\beta_1}>e^{-a\beta_1}$ por lo tanto
$$g(a)=\frac{1}{1+Ce^{-\beta_1a}}>\frac{1}{1+Ce^{-\beta_1b}}=g(b)$$   

Así tenemos que la función $g(X_i)$ es monótona decreciente si $\beta_1<0$

3.- Ahora veamos que es simétrica:
$$
\begin{array}{rl}
g(-\frac{\beta_0}{\beta_1}-u) & = \frac{1}{1+e^{-\left(\beta_0+\beta_1 \left[-\frac{\beta_0}{\beta_1}-u\right]\right)}}\\
g(-\frac{\beta_0}{\beta_1}-u) & = \frac{1}{1+e^{-\beta_0-\beta_1 \left(-\frac{\beta_0}{\beta_1}-u\right)}}\\
g(-\frac{\beta_0}{\beta_1}-u) & = \frac{1}{1+e^{-\beta_0+\beta_1 \frac{\beta_0}{\beta_1}+\beta_1u}}\\
g(-\frac{\beta_0}{\beta_1}-u) & = \frac{1}{1+e^{-\beta_0+\beta_0+\beta_1u}}\\
g(-\frac{\beta_0}{\beta_1}-u) & = \frac{1}{1+e^{\beta_1u}}\\
\end{array}
$$

Por otro lado tenemos:
$$
\begin{array}{rl}
1-g(-\frac{\beta_0}{\beta_1}+u) & = 1-\frac{1}{1+e^{-\left(\beta_0+\beta_1 \left[-\frac{\beta_0}{\beta_1}+u\right]\right)}}\\
1-g(-\frac{\beta_0}{\beta_1}+u) & = 1-\frac{1}{1+e^{-\beta_0-\beta_1 \left(-\frac{\beta_0}{\beta_1}+u\right)}}\\
1-g(-\frac{\beta_0}{\beta_1}+u) & = 1-\frac{1}{1+e^{-\beta_0+\beta_1 \frac{\beta_0}{\beta_1}-\beta_1u}}\\
1-g(-\frac{\beta_0}{\beta_1}+u) & = 1-\frac{1}{1+e^{-\beta_0+\beta_0-\beta_1u}}\\
1-g(-\frac{\beta_0}{\beta_1}+u) & = 1-\frac{1}{1+e^{-\beta_1u}}\\
1-g(-\frac{\beta_0}{\beta_1}+u) & = \frac{1+e^{-\beta_1u}-1}{1+e^{-\beta_1u}}\\
1-g(-\frac{\beta_0}{\beta_1}+u) & = \frac{e^{-\beta_1u}}{1+e^{-\beta_1u}}\\
1-g(-\frac{\beta_0}{\beta_1}+u) & = \frac{1}{1+e^{\beta_1u}}\\
\end{array}
$$

Obteniendo finalmente que
$$g\left(-\frac{\beta_0}{\beta_1}+u\right)=1-g\left(-\frac{\beta_0}{\beta_1}+u\right)\ \forall \ u \in \mathbb{R}$$

Generalmente la función $g(X_i)$ se escribe de la siguiente manera:
$$p_i=\frac{1}{1+e^{-(\beta_0+\beta_1X_i)}}=\frac{1}{1+\frac{1}{e^{\beta_0+\beta_1X_i}}}=\frac{e^{\beta_0+\beta_1X_i}}{1+e^{\beta_0+\beta_1X_i}}$$

Entonces 
$$1-p_i=1-\frac{e^{\beta_0+\beta_1X_i}}{1+e^{\beta_0+\beta_1X_i}}=\frac{1}{1+e^{\beta_0+\beta_1X_i}}$$

Luego 
$$\frac{p_i}{1-p_i}=\frac{\frac{e^{\beta_0+\beta_1X_i}}{1+e^{\beta_0+\beta_1X_i}}}{\frac{1}{1+e^{\beta_0+\beta_1X_i}}}=e^{\beta_0+\beta_1X_i}$$

Este cociente es conocido como cociente de probabilidad.

Si aplicamos la función logaritmo tenemos:
$$ln\left(\frac{p_i}{1-p_i}\right)=ln(e^{\beta_0+\beta_1X_i})=\beta_0+\beta_1X_i$$

La función $ln\left(\frac{p_i}{1-p_i}\right)$ es llamada $logit$ de $p_i$, se denota
$$
\begin{array}{rl}
logit(pi) & = \beta_0+\beta_1X_i\\
& = ln(p_i)- ln(1-p_i)
\end{array}
$$

De donde notamos que la variable $logit(p_i)$ mide la diferencia entre las probabilidades de éxito ($p_i$) y fracaso ($1-p_i$) en escala logaritmica.

### Modelo de Regresión Logística

Acabamos de ver la relación que hay entre el modelo de regresión logística y el modelo de regresión lineal. Como conclusión podemos decir que el modelo de regresión logistica consiste en una variable dependiente (explicada) usualmente denotada por $Y$ y una o varias variables independientes denotadas por $X_i$.
$$
\mathbb{E}(Y=1|x_1,x_2,\dots,x_p) = \frac{e^{\beta_0+\beta_1x_1+\dots+\beta_px_p}}{1+e^{\beta_0+\beta_1x_1+\dots+\beta_px_p}}
$$

Como la variable dependiente toma dos valores, a saber, $Y=0 \vee  Y=1$ entonces $$\mathbb{E}(Y=1|x_1,x_2,\dots,x_p)=P(Y=1|x_1,x_2,\dots,x_p)$$

Es decir, la esperanza de que $Y=1$ dado un conjunto de valores de las variables independientes $x_1,x_2,\dots,x_p$ es igual a la probabilidad de que $Y=1$ dado el conjunto de valores de las variables independientes $x_1,x_2,\dots,x_p$. Veamos porque:
$$
\begin{array}{rl}
\mathbb{E}(Y|x_1,x_2,\dots,x_p) &= 1P(Y=1|x_1,x_2,\dots,x_p) + 0 P(Y=0|x_1,x_2,\dots,x_p)\\
& = P(Y=1|x_1,x_2,\dots,x_p)\\
& = \mathbb{E}(Y=1|x_1,x_2,\dots,x_p)
\end{array}
$$

En el caso de una regresión logística simple (una variable respuesta dado una variable explicativa) la ecuación sería
$$\mathbb{E}(Y|x) = P(Y=1|x) = \frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}$$

A esta expresión la vamos a llamar $\pi (x)$
$$\pi(x)=\frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}$$

Por ejemplo si $\beta_0=-7$ y $\beta_1=3$ para 
$$
\pi(x)= \frac{e^{-7+3x}}{1+e^{-7+3x}}
$$

Donde el gráfico es:
```{r}
E_y<-c()
beta0<- -7
beta1<- 3
for(x in seq(0,5,by=0.01)) {
  p_i<-exp(beta0+beta1*x)
  E_y<-c(E_y, p_i/(1+p_i))
}
plot(seq(0,5,by=0.01),E_y, type = "l", 
     xlab = TeX('$x$'), 
     ylab = TeX('$E(Y=1 | x)$'), 
     main = TeX('Regresión Logística $\\beta_0= -7$ y $\\beta_1= 3$'))
```

Aplicandole la transformación $logit$ al cociente de probabilidad $\frac{\pi(x)}{1-\pi(x)}$ tenemos
$$
\begin{array}{rl}
g(x) & = ln\left(\frac{\pi(x)}{1-\pi(x)}\right)\\
g(x) & = ln(\pi(x))-ln(1-\pi(x))\\
g(x) & = ln\left(\frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}} \right) - ln\left(1-\frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}} \right)\\
g(x) & = ln(e^{\beta_0+\beta_1x}) - ln(1+e^{\beta_0+\beta_1x}) -ln\left(\frac{1}{1+e^{\beta_0+\beta_1x}}\right)\\
g(x) & = ln(e^{\beta_0+\beta_1x}) - ln(1+e^{\beta_0+\beta_1x}) -ln(1) +ln(1+e^{\beta_0+\beta_1x})\\
g(x) & = ln(e^{\beta_0+\beta_1x})\\
g(x) & = \beta_0+\beta_1x\\
\end{array}
$$

Ésta transformación es importante ya que tiene muchas propiedades de un modelo de regresión lineal. Es lineal en sus parámetros, puede ser continua y puede tomar valores entre $-\infty$ y $\infty$ dependiendo de los valores que puede tomar $x$

Una diferencia muy importante entre el modelo de regresión lineal y el de regresión logistica es el siguiente:

- Para el modelo de regresión lineal nosotros suponemos que $y=\mathbb{E}(Y|x)+\epsilon$, donde asumimos que $\epsilon$ es un error que representa la desviación de la media condicional, generalmente se asume que tiene una distribución Normal con media cero y varianza constante, esta suposición conlleva a que $y$ sea Normal con media $\mathbb{E}(Y|x)$ y varianza constante. 

- En el caso con la variable de salida dicotómica suponemos que $y=\pi(x)+\epsilon$, donde $\epsilon$ solo podrá tomar dos valores:

    - Si $y=1$ entonces $\epsilon=1-\pi(x)$ con probabilidad $\pi(x)$
    - Si $y=0$ entonces $\epsilon= \pi(x)$ con probabilidad $1-\pi(x)$
    
Entonces la media de $\epsilon$ es 
$$\begin{array}{rl}
\mathbb{E}(\epsilon) & = (1-\pi(x))P(\epsilon=1-\pi(x))+\pi(x)P(\epsilon=\pi(x))\\
\mathbb{E}(\epsilon) & = (1-\pi(x))\pi(x)+\pi(x)(1-\pi(x))\\
\mathbb{E}(\epsilon) & = 0
\end{array}
$$

y la varianza es
$$
\begin{array}{rl}
Var(\epsilon) & = \mathbb{E}(\epsilon^2)-\mathbb{E}(\epsilon)^2=\mathbb{E}(\epsilon^2)\\
Var(\epsilon) & = (1-\pi(x))^2 P(\epsilon^2=(1-\pi(x))^2)+\pi(x)^2P(\epsilon^2=\pi(x)^2)\\
Var(\epsilon) & = (1-\pi(x))^2 \pi(x)+\pi(x)^2 (1-\pi(x))\\
Var(\epsilon) & = (1-\pi(x))((1-\pi(x)) \pi(x)+\pi(x)^2)\\
Var(\epsilon) & = (1-\pi(x))(\pi(x)-\pi(x)^2+\pi(x)^2)\\
Var(\epsilon) & = (1-\pi(x))\pi(x)\\
\end{array}
$$
Entonces tenemos que los errores se distribuyen Binomiales, y por lo tanto la variable de respuesta tiene una distribución Binomial.

### Ajuste del modelos de regresión logística

Tenemos que encontrar los parámetros desconocidos $\beta_0$ y $\beta_1$, el método más usado para estimarlos es el de mínimos cuadrados. En los modelos de regresión lineal la manera de calcular estos parámetos es maximizando la función de máxima verosimilitud (esta función expresa la probabilidad de los datos observados como una función de parámetros desconocidos). En el caso de los modelos de regresión logística tenemos:

Como la variable de respuesta $Y$ es dicotómica, toma los valores 0 y 1. Entonces tenemos que $\pi(x)=\frac{e^{\beta_0+\beta_1x}}{1+e^{\beta_0+\beta_1x}}$ proporciona la probabilidad de que $Y=1$ dado $x$, para el vector de parámetros $\beta=(\beta_0,\beta_1)$. Así $1-\pi(x)$ nos da la probabilidad de que $Y=0$ dado $x$. 

Para un par $(x_i,y_i)$:

  - Si $y_i=1$ la contribución que da la función de verosimilitud es $\pi(x_i)$
  - Si $y_i=0$ la contribución que da la función de verosimilitud es $1-\pi(x_i)$

Para expresar esta contribución a la función de verosimilitud del par $(x_i,y_i)$ tenemos la siguiente expresión:
$$\pi(x_i)^{y_i}(1-\pi(x_i))^{1-y_i}$$

Asumiendo independencia en los datos observados, tenemos que la función de verosimilitud es el producto de la ecuación anterior de todos los $n$ datos
$$l(\beta)=\prod_{i=1}^n \pi(x_i)^{y_i}(1-\pi(x_i))^{1-y_i}$$

Como esta función es difícil de manejar aplicamos $ln$ y luego maximizamos, derivando con respecto a los parámetros e igualando a cero
$$
\begin{array}{rl}
L(\beta) & = ln(l(\beta))\\
L(\beta) & = ln \left( \prod_{i=1}^n \pi(x_i)^{y_i}(1-\pi(x_i))^{1-y_i} \right)\\
L(\beta) & = \sum_{i=1}^n  ln(\pi(x_i)^{y_i}(1-\pi(x_i))^{1-y_i})\\
L(\beta) & = \sum_{i=1}^n  ln(\pi(x_i)^{y_i})+ln((1-\pi(x_i))^{1-y_i})\\
L(\beta) & = \sum_{i=1}^n  y_iln(\pi(x_i))+(1-y_i)ln(1-\pi(x_i))\\
L(\beta) & = \sum_{i=1}^n  y_iln(\pi(x_i))+ln(1-\pi(x_i)) -y_iln(1-\pi(x_i))\\
L(\beta) & = \sum_{i=1}^n  y_i\left( ln(\pi(x_i))-ln(1-\pi(x_i)) \right)+ln(1-\pi(x_i))\\
L(\beta) & = \sum_{i=1}^n  y_i ln\left( \frac{\pi(x_i)}{1-\pi(x_i)} \right)+ln(1-\pi(x_i))\\
\end{array}
$$

Recordemos que $ln\left( \frac{\pi(x_i)}{1-\pi(x_i)} \right)=\beta_0+\beta_1x_i$ y $1-\pi(x_i)=1-\frac{e^{\beta_0+\beta_1x_i}}{1+e^{\beta_0+\beta_1x_i}}=\frac{1}{1+e^{\beta_0+\beta_1x_i}}$.

Sustituyendo ésto:
$$
\begin{array}{rl}
L(\beta) & = \sum_{i=1}^n  y_i (\beta_0+\beta_1x_i)+ln\left(\frac{1}{1+e^{\beta_0+\beta_1x_i}}\right)\\
L(\beta) & = \sum_{i=1}^n  y_i (\beta_0+\beta_1x_i)+ln(1)-ln(1+e^{\beta_0+\beta_1x_i})\\
L(\beta) & = \sum_{i=1}^n  y_i (\beta_0+\beta_1x_i)-ln(1+e^{\beta_0+\beta_1x_i})\\
\end{array}
$$

Ahora está en función de los parámetros $\beta_0$ y $\beta_1$, derivando con respecto a $\beta_0$:
$$
\begin{array}{rl}
\frac{\partial L(\beta)}{\partial \beta_0} & = \sum_{i=1}^n y_i -\frac{1}{1+e^{\beta_0+\beta_1x_i}} e^{\beta_0+\beta_1x_i}\\
\frac{\partial L(\beta)}{\partial \beta_0} & = \sum_{i=1}^n y_i -\frac{e^{\beta_0+\beta_1x_i}}{1+e^{\beta_0+\beta_1x_i}}\\
\frac{\partial L(\beta)}{\partial \beta_0} & = \sum_{i=1}^n y_i -\pi(x_i)\\
& \implies \sum_{i=1}^n y_i -\pi(x_i) = 0 \qquad (3)
\end{array}
$$

Derivando con respecto a $\beta_1$:
$$
\begin{array}{rl}
\frac{\partial L(\beta)}{\partial \beta_1} & = \sum_{i=1}^n y_ix_i -\frac{1}{1+e^{\beta_0+\beta_1x_i}} e^{\beta_0+\beta_1x_i}x_i\\
\frac{\partial L(\beta)}{\partial \beta_1} & = \sum_{i=1}^n y_ix_i -\frac{e^{\beta_0+\beta_1x_i}}{1+e^{\beta_0+\beta_1x_i}} x_i\\
\frac{\partial L(\beta)}{\partial \beta_1} & = \sum_{i=1}^n y_ix_i -\pi(x_i) x_i\\
\frac{\partial L(\beta)}{\partial \beta_1} & = \sum_{i=1}^n x_i\left( y_i -\pi(x_i) \right)\\
& \implies \sum_{i=1}^n x_i\left( y_i -\pi(x_i) \right) = 0 \qquad (4)
\end{array}
$$

Las ecuaciones obtenidas no son lineales en los parámetros, por lo tanto hay que recurrir a otros métodos para encontrar valores aproximados $\hat \beta$ de $\beta$.

De la ecuación $(3)$ tenemos:
$$\sum_{i=1}^n y_i= \sum_{i=1}^n \hat\pi(x_i)$$

Es decir, la suma de los valores observados es igual a la suma de los valores esperados.

Cuando se tienen más de una variable explicativa aplicamos el mismo procedimiento de derivar con respecto a cada parámetro e igularlas a cero, obteniendo $\hat\beta_0,\hat\beta_1,\dots,\hat\beta_p$ de los parámetros $\beta_0,\beta_1,\dots,\beta_p$. En este caso el modelo de regresión logistica estimado queda
$$
\hat{y}=\mathbb{E}(Y=1|x_1,x_2,\dots,x_p) = P(Y=1|x_1,x_2,\dots,x_p) = \frac{e^{\hat\beta_0+\hat\beta_1x_1+\dots+\hat\beta_px_p}}{1+e^{\hat\beta_0+\hat\beta_1x_1+\dots+\hat\beta_px_p}}
$$

Una vez obtenidos los parámetros estimados podemos calcular la probabilidad cuando las variables independientes toman valores específicos.

### Prueba de significancia para los parámetros

En todos los modelos de regresión es importante entender y calcular cúal es el nivel de aporte o significancia de cada variable al modelo de regresión. La pregunta es ¿Qué tanto aporta una variable para predecir la variable de respuesta?.

Para responder esto se predice la variable respuesta con un modelo que tenga la variable explicativa en cuestión y luego se compara con un modelo que no tenga dicha variable explicativa. La función que se usa para comparar depende del problema particular, si el valor de predicción con la variable en el modelo es mejor, predice mejor a la variable respuesta que el modelo sin la variable explicativa en cuestion, entonces nosotros decimos que la variable explicativa estudiada es **significativa**.

En la regresión lineal una prueba para el nivel de significancia de los coeficientes es la tabla de *análisis de varianza*, esta divide la suma total de las desviaciones de las observaciones sobre la media al cuadrado en dos partes:

- La suma de las desviaciones de las observaciones sobre la regresión lineal al cuadrado ó suma al cuadrado de los residuos $SSE$.

- La suma al cuadrado de los valores de predicción, basados en el modelo de regresión, sobre la media de las variables dependientes o suma al cuadrado de la regresión $SSR$

Si $y_i$ es el valor observado y $\hat y_i$ denota su valor de predicción del individuo $y_i$ dado el modelo, entonces el estadístico usado para hacer la comparación es:
$$SSE=\sum_{i=1}^n (y_i-\hat y_i)^2$$

Si el modelo no contiene a la variable independiente el único parámetro sería $\beta_0$ y en este caso su estimador es $\hat \beta_0 = \bar y$, es decir la media de la variable respuesta, así $\hat y_i=\bar y$ y $SSE$ sería igual a la suma total al cuadrado. Cuando se incluye a la variable independiente cualquier decrecimiento en $SSE$ se debe al factor del coeficiente de la variable independiente, cuando este es distinto de cero. El cambio en el valor de $SSE$ se debe a la variabilidad de la regresión, que denotaremos por $SSR$, que es: 
$$SSR=\sum_{i=1}^n (y_i-\bar y)^2 - \sum_{i=1}^n (y_i-\hat y_i)^2$$

Un valor grande en $SSR$ implica que la variable independiente es importante, y uno pequeño implica que no es importante para predecir a la variable de respuesta.

En la regresión logística, la comparación del valor observado con el valor predicho esta basado en el logaritmo de la función de verosimilitud. 

La comparación del valor observado y el predictivo es usando la función de verosimilitud mediante la siguiente expresión:
$$D=-2ln\left[ \frac{verosimilitud\ con\ el\ modelo\ ajustado}{verosimilitud\ con\ el\ modelo\ saturado}\right]$$
Donde un modelo saturado es uno que contiene tantos parámetros como valores observados. En el modelo saturado todos los datos son parámetros, entonces:
$$L(modelo\ saturado)=\prod_{i=1}^n y_i^{y_i}(1-y_i)^{1-y_i}$$
Como $y_i=0$ ó $y_i=1$ tenemos:

- Si $y_i=0\implies y_i^{y_i}(1-y_i)^{1-y_i}=0^0\times 1^1= 1\times 1=1$

- Si $y_i=1\implies y_i^{y_i}(1-y_i)^{1-y_i}=1^1\times 0^0= 1\times 1=1$

Así concluimos que la verosimilitud del modelo saturado es 1 
$$L(modelo\ saturado)=1$$

Por lo tanto 
$$
\begin{array}{rl}
D & =-2ln\left[ \frac{verosimilitud\ con\ el\ modelo\ ajustado}{1}\right]\\
D & =-2ln(verosimilitud\ con\ el\ modelo\ ajustado)\\
D & = -2\sum_{i=1}^n  y_i ln\left(\frac{\hat\pi(x_i)}{y_i}\right)+(1-y_i)ln\left( \frac{1+\hat\pi(x_i)}{1-y_i}\right)
\end{array}
$$

A este estadístico se le llama la **desvianza**. Tiene una distribución asintótica como la Chi Cuadrado. 

En particular si queremos saber el nivel de significancia de una variable independiente comparamos el valor del estadístico $D$ con y sin la variable independiente
$$G=D(\text{modelo sin la variable})-D(\text{modelo con la variable})$$

Como en ambos estadísticos se tiene la verosimilitud del modelo saturado tenemos que:
$$
\begin{array}{rl}
G & = -2ln(\text{verosimilitud sin la variable})+2ln(\text{verosimilitud con la variable})\\
G & = -2[ ln(\text{verosimilitud sin la variable})-ln(\text{verosimilitud con la variable})]\\
G & = -2ln\left(\frac{\text{modelo sin la variable}}{\text{modelo con la variable}} \right)
\end{array}
$$ 

Veamos el caso de una variable independiente, la forma que tiene el estimador de máxima verosimilitud es:
$$
\begin{array}{rl}
L(\beta_0) & = \sum_{i=1}^n  y_i ln\left( \frac{\pi(x_i)}{1-\pi(x_i)} \right)+ln(1-\pi(x_i))\\
L(\beta_0) & = \sum_{i=1}^n  y_i (\beta_0)+ln\left(\frac{1}{1+e^{\beta_0}}\right)\\
L(\beta_0) & = \sum_{i=1}^n  y_i (\beta_0)+ln(1)-ln(1+e^{\beta_0})\\
L(\beta_0) & = \sum_{i=1}^n  y_i (\beta_0)-ln(1+e^{\beta_0})
\end{array}
$$

Así el estimador $\hat \beta_0$ quedaría
$$
\begin{array}{rl}
\frac{\partial L(\beta_0)}{\partial \beta_0} & = \frac{\partial}{\partial \beta_0} \sum_{i=1}^n  y_i \beta_0-ln(1+e^{\beta_0})\\
\frac{\partial L(\beta_0)}{\partial \beta_0} & = \sum_{i=1}^n  y_i -\frac{1}{1+e^{\beta_0}}e^{\beta_0}=0\\
& \implies \frac{\sum_{i=1}^n  y_i(1+e^{\beta_0})-e^{\beta_0}}{1+e^{\beta_0}}=0\\
& \sum_{i=1}^n  y_i(1+e^{\beta_0})-e^{\beta_0} =0\\
& \sum_{i=1}^n  y_i+y_ie^{\beta_0}-e^{\beta_0} =0\\
& \sum_{i=1}^n  y_i+e^{\beta_0}(y_i-1) =0\\
& \sum_{i=1}^n  y_i+e^{\beta_0}\sum_{i=1}^n(y_i-1) =0\\
& \implies e^{\hat \beta_0}= \frac{-\sum_{i=1}^n  y_i}{\sum_{i=1}^n(y_i-1)}\\
& \implies \hat \beta_0 = ln \left( \frac{-\sum_{i=1}^n  y_i}{\sum_{i=1}^n(y_i-1)}\right)=ln \left( \frac{\sum_{i=1}^n  y_i}{\sum_{i=1}^n(1-y_i)}\right)\\
\end{array}
$$

Sustituyendo en el modelo $\pi(x_i)^{y_i}(1-\pi(x_i))^{1-y_i}$ tenemos:
$$
\begin{array}{rl}
\pi(x_i)^{y_i}(1-\pi(x_i))^{1-y_i} & = \left( \frac{e^{\hat \beta_0}}{1+e^{\hat \beta_0}} \right)^{y_i}\left(1-\frac{e^{\hat \beta_0}}{1+e^{\hat \beta_0}}\right)^{1-y_i}\\
& = \left( \frac{e^{\hat \beta_0}}{1+e^{\hat \beta_0}} \right)^{y_i}\left(\frac{1}{1+e^{\hat \beta_0}}\right)^{1-y_i}\\
& = \left( \frac{\frac{-\sum_{i=1}^n  y_i}{\sum_{i=1}^n(y_i-1)}}{1+\frac{-\sum_{i=1}^n  y_i}{\sum_{i=1}^n(y_i-1)}} \right)^{y_i}\left(\frac{1}{1+\frac{-\sum_{i=1}^n  y_i}{\sum_{i=1}^n(y_i-1)}}\right)^{1-y_i}\\
& = \left( \frac{\frac{-\sum_{i=1}^n  y_i}{\sum_{i=1}^n(y_i-1)}}{\frac{\sum_{i=1}^n(y_i-1)-\sum_{i=1}^n  y_i}{\sum_{i=1}^n(y_i-1)}} \right)^{y_i}\left(\frac{1}{\frac{\sum_{i=1}^n(y_i-1)-\sum_{i=1}^n  y_i}{\sum_{i=1}^n(y_i-1)}}\right)^{1-y_i}\\
& = \left( \frac{-\sum_{i=1}^n y_i}{\sum_{i=1}^n(y_i-1)-\sum_{i=1}^n  y_i} \right)^{y_i}\left(\frac{\sum_{i=1}^n(y_i-1)}{\sum_{i=1}^n(y_i-1)-\sum_{i=1}^n  y_i}\right)^{1-y_i}\\
& = \left( \frac{-\sum_{i=1}^n y_i}{-n} \right)^{y_i}\left(\frac{\sum_{i=1}^n(y_i-1)}{-n}\right)^{1-y_i}\\
& = \left( \frac{\sum_{i=1}^n y_i}{n} \right)^{y_i}\left(-\frac{\sum_{i=1}^n(y_i-1)}{n}\right)^{1-y_i}\\
& = \left( \frac{\sum_{i=1}^n y_i}{n} \right)^{y_i}\left(\frac{\sum_{i=1}^n(1-y_i)}{n}\right)^{1-y_i}\\
\end{array}
$$
Si $n_1=\sum_{i=1}^n y_i$ y $n_0=\sum_{i=1}^n 1- y_i$
$$\pi(x_i)^{y_i}(1-\pi(x_i))^{1-y_i} = \left( \frac{n_1}{n} \right)^{y_i}\left(\frac{n_0}{n}\right)^{1-y_i}
$$

Por lo tanto, la verosimilitud del modelo sin la variable es:
$$
\begin{array}{rl}
l(\beta) & =  ln(\prod_{i=1}^n \left( \frac{n_1}{n} \right)^{y_i}\left(\frac{n_0}{n}\right)^{1-y_i})\\
l(\beta) & =  ln( \left( \frac{n_1}{n} \right)^{\sum_{i=1}^n y_i}\left(\frac{n_0}{n}\right)^{\sum_{i=1}^n 1-y_i})\\
l(\beta) & =  ln( \left( \frac{n_1}{n} \right)^{n_1}\left(\frac{n_0}{n}\right)^{n_0})\\
\end{array}
$$

Sustituyendo esto, y la verosimilitud de un modelo con una variable, tenemos:
$$G = -2ln\left(\frac{\left( \frac{n_1}{n} \right)^{n_1}\left(\frac{n_0}{n}\right)^{n_0}}{\prod_{i=1}^n \hat \pi(x_i)^{y_i}(1-\hat \pi(x_i))^{1-y_i}} \right)$$ 

Otra forma de escribir esto es:
$$
\begin{array}{rl}
G & = -2ln\left(\left( \frac{n_1}{n} \right)^{n_1}\left(\frac{n_0}{n}\right)^{n_0}\right) +2ln\left( {\prod_{i=1}^n \hat \pi(x_i)^{y_i}(1-\hat \pi(x_i))^{1-y_i}} \right)\\
G & = 2\left(ln\left( {\prod_{i=1}^n \hat \pi(x_i)^{y_i}(1-\hat \pi(x_i))^{1-y_i}}\right) - ln\left(\left( \frac{n_1}{n} \right)^{n_1}\left(\frac{n_0}{n}\right)^{n_0}\right) \right)\\
G & = 2\left(ln\left( {\prod_{i=1}^n \hat \pi(x_i)^{y_i}(1-\hat \pi(x_i))^{1-y_i}}\right) - [n_1ln\left( \frac{n_1}{n} \right)+ n_0 ln \left(\frac{n_0}{n}\right)] \right)\\
G & = 2\left(ln\left( {\prod_{i=1}^n \hat \pi(x_i)^{y_i}(1-\hat \pi(x_i))^{1-y_i}}\right) - [n_1ln(n_1)-n_1ln(n) + n_0 ln(n_0) - n_0 ln(n)] \right)\\
G & = 2\left(ln\left( {\prod_{i=1}^n \hat \pi(x_i)^{y_i}(1-\hat \pi(x_i))^{1-y_i}}\right) - [n_1ln(n_1)-ln(n)(n_1+n_0) + n_0 ln(n_0)] \right)\\
G & = 2\left(ln\left( {\prod_{i=1}^n \hat \pi(x_i)^{y_i}(1-\hat \pi(x_i))^{1-y_i}}\right) - [n_1ln(n_1)-nln(n) + n_0 ln(n_0)] \right)\\
\end{array}
$$ 

Bajo la hipótesis nula que $\beta_1$ es igual a 0, el estadístico $G$ sigue una distribución Chi-Cuadrado con 1 grado de libertad $\chi^2_1$. El $p-valor$ asociado a esta prueba es de la forma $P(\chi^2_1 > G)< \alpha$, donde $\alpha$ es el nivel de significancia.

#### Prueba de hipótesis global

Para probar si los 

#### Prueba de hipótesis por variable

### Cociente de verosimilitudes

La verosimilitud de un evento se define como la razón entre la probabilidad de que el evento sea exitoso (ocurra) y la probabilidad de que el evento falle (no ocurra).

$$
Verosimilitud=V=\frac{P(Y=1|x_1,x_2,\dots,x_p)}{P(Y=0|x_1,x_2,\dots,x_p)}=\frac{P(Y=1|x_1,x_2,\dots,x_p)}{1-P(Y=1|x_1,x_2,\dots,x_p)}
$$

El cociente de verosimilitudes mide el cambio que tiene sobre las probabilidades el aumento en una unidad en una de las variables independientes del modelo.

Por otra parte el cociente de verosimilitudes mide el efecto o impacto que genera el cambio en una unidad de una de las variables independientes.

Entonces el cociente de verosimilitud se calcula como sigue

$$
\text{Cociente de verosimilitud}=L_{x_i}=\frac{V_{x_i=k+1}}{V_{x_i=k}}=\displaystyle\frac{\frac{P(Y=1|x_1,x_2,\dots,x_i=k+1,\dots,x_p)}{1-P(Y=1|x_1,x_2,\dots,x_i=k+1,\dots,x_p)}}{\frac{P(Y=1|x_1,x_2,\dots,x_i=k,\dots,x_p)}{1-P(Y=1|x_1,x_2,\dots,x_i=k,\dots,x_p)}}
$$

Es importante notar que esta razón de verosimilitudes se calcula manteniendo el resto de las variables independientes constantes.

Sin embargo hay un método más sencillo para calcular la razón de verosimilitudes utilizando el valor estimado del parámetro.

$$
\text{Cociente de verosimilitud}=L_{x_i}=e^{\beta_i}
$$

Una vez obtenidos los valores estimados de los parámetros del modelo es posible calcular el cociente de verosimilitud de cada variable. Además nos permite calcular los cambios en el cociente de verosimilitud cuando se presentan cambios mayores a una unidad en una variable independiente mientras que el resto de las variables independientes permanece constante.



$$
\text{Cociente de verosimilitud}=L_{\Delta(x_i)}=e^{\Delta(x_i)\beta_i}
$$

Finalmente, el cociente de verosimilitud permite calcular y comparar la verosimilitud de dos eventos diferentes. Si la verosimilitud es igual a $1$ indica que los dos eventos tienen la misma verosimilitud, si por otra parte la variable independiente es significativa y tiene efecto positivo sobre la probabilidad de que el evento ocurra, el cociente de verosimilitud debe ser mayor que $1$.

### Transformación *logit*

$$
\ln(V)=\beta_0+\beta_1x_1+\dots+\beta_px_p
$$

$$
g(x_1+\dots+x_p)=\beta_0+\beta_1x_1+\dots+\beta_px_p
$$

$$
\mathbb{E}(Y=1|x_1,x_2,\dots,x_p) = \displaystyle\frac{e^{g(x_1+\dots+x_p)}}{1+e^{g(x_1+\dots+x_p)}}
$$

$$
\hat{g}(x_1+\dots+x_p)=\hat\beta_0+\hat\beta_1x_1+\dots+\hat\beta_px_p
$$

$$
\displaystyle\frac{e^{g(x_1+\dots+x_p)}}{1+e^{g(x_1+\dots+x_p)}}=\frac{e^{\hat\beta_0+\hat\beta_1x_1+\dots+\hat\beta_px_p}}{1+e^{\hat\beta_0+\hat\beta_1x_1+\dots+\hat\beta_px_p}}
$$






